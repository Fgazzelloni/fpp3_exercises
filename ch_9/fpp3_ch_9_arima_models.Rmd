---
title: | 
  | Chapter 7 - Time Series Regression Models"
  | Forecasting: Principles and Practice
author: "R. J. Serrano"
date: "04/18/2023"
output: 
     slidy_presentation:
          highlight: tango
          theme: flatly
          df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.width = 10, fig.height = 8)
```

# ARIMA models ---

Learning objectives:

-   Stationarity and differencing

-   Non-seasonal ARIMA models

-   Estimation and order selection

-   ARIMA modeling in R

-   Forecasting

-   Seasonal ARIMA models

-   ARIMA vs ETS

```{r echo = FALSE}
suppressMessages(library(tidyverse))
library(fpp3)
library(plotly)
theme_set(theme_minimal())
```

# Introduction ---

ARIMA models provide another approach to time series forecasting. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

# 9.1 - Stationarity and differencing ---

A stationary time series is one whose statistical properties do not depend on the time at which the series is observed.

Thus, a stationary time series exhibits the following characteristics:

-   Roughly horizontal (no trend)

-   Constant variance

-   No long-term predictable patterns

#

![Figure 9.1: Which of this series are stationary?](../img/fig_9_1_stationary.png){heigth="1000px," width="800px"}

# Differencing

In Figure 9.1, note that the Google stock price was non-stationary in panel (a), but the daily changes were stationary in panel (b). This shows one way to make a non-stationary time series stationary — compute the differences between consecutive observations. This is known as **differencing**.

```{r google_stock_price}
google_2018 <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) == 2018)

# original time series
google_2018 |> 
     autoplot(Adj_Close) + 
     labs(title = 'Google Closing Stock Price ($USD)')

google_2018 |> ACF(Close) |>
  autoplot() + labs(subtitle = "Google closing stock price")

google_2018 |> ACF(difference(Close)) |>
  autoplot() + labs(subtitle = "Changes in Google closing stock price")
```

```{r ljung_box_test}
google_2018 |>
  mutate(diff_close = difference(Close)) |>
  features(diff_close, ljung_box, lag = 10)
```

# Random walk model ----

A time series is said to follow a random walk process if the predicted value of the series in one period is equivalent to the value of the series in the previous period plus a random error. Source: https://analystprep.com/study-notes/cfa-level-2/random-walk-process/

If differenced series is white noise with zero mean:

\begin{block}{}
\centerline{$y_t-y_{t-1}=\varepsilon_t$ \hspace{0.4cm} or \hspace{0.4cm} $y_t=y_{t-1}+\varepsilon_t$}
\end{block}\vspace*{-0.3cm}
where $\varepsilon_t \sim NID(0,\sigma^2)$.

Random walk models are widely used for non-stationary data, particularly financial and economic data. Random walks typically have:

-   long periods of apparent trends up or down

-   sudden and unpredictable changes in direction

# Seasonal differening ---- 

A seasonal difference is the difference between an observation and the corresponding observation from the previous year.\pause
$$
 y'_t = y_t - y_{t-m}
$$
Antidiabetic drug sales

```{r}
a10 <- PBS |>
  filter(ATC2 == "A10") |>
  summarise(Cost = sum(Cost) / 1e6)

a10 |> autoplot(
  Cost
) + 
     labs(title = 'Austrialia monthly scripts for A10 (antidiabetic) drugs sold')
```

```{r}
a10 |> autoplot(
  log(Cost)
) + 
     labs(title = 'Austrialia monthly scripts for A10 (antidiabetic) drugs sold')
```

```{r}
a10 |> autoplot(
  log(Cost) |> difference(12)
) + 
     labs(title = 'Annual change in monthly scripts for A10 (antidiabetic) drugs sold')
```

# Unit root test ----

One way to determine more objectively whether differencing is required is to use a *unit root test*. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required.

For example, let us apply it to the Google stock price data.

```{r}
google_2018 |>
  features(Close, unitroot_kpss)
```

The p-value is less than 0.05, so the null hypothesis (time series is stationary) is rejected in favor of the alternate hypothesis (time series is **NOT** stationary).

We can difference the time series and apply the test again.
```{r}
google_2018 |>
  mutate(diff_close = difference(Close)) |>
  features(diff_close, unitroot_kpss)
```

This time, the p-value is greater than 0.05, so we can conclude that the time series appears stationary.

# 9.2 - Backshift notation

A very useful notational device is the backward shift operator, $B$, which is used as follows:
$$
  B y_{t} = y_{t - 1}
$$\pause


In other words, $B$, operating on $y_{t}$, has the effect of **shifting the data back one period**. \pause

Two applications of $B$ to $y_{t}$ **shifts the data back two periods**:
$$
  B(By_{t}) = B^{2}y_{t} = y_{t-2}
$$\pause


# 9.3 - Autoregressive models

In a multiple regression model, introduced in Chapter 7, we forecast the variable of interest using a linear combination of predictors. In an autoregression model, we forecast the variable of interest using a linear combination of *past values of the variable*. The term *auto*regression indicates that it is a regression of the variable **against itself**.

\begin{block}{Autoregressive (AR) models:}
$$
  y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \varepsilon_{t},
$$\pause

where $\varepsilon_t$ is white noise. This is a multiple regression with \textbf{lagged values} of $y_t$ as predictors.
\end{block}

## AR(1) model

\begin{block}{}
  \centerline{$y_{t} = c + \phi_1 y_{t - 1} + \varepsilon_{t}$}
\end{block}

* When $\phi_1=0$, $y_t$ is **equivalent to WN**
* When $\phi_1=1$ and $c=0$, $y_t$ is **equivalent to a RW**
* When $\phi_1=1$ and $c\ne0$, $y_t$ is **equivalent to a RW with drift**
* When $\phi_1<0$, $y_t$ tends to **oscillate between positive and negative values**.

# 9.4 - Moving average models

\begin{block}{Moving Average (MA) models:}
$$
  y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},
$$
where $\varepsilon_t$ is white noise.
This is a multiple regression with \textbf{past \emph{errors}} as predictors. \emph{Don't confuse this with moving average smoothing!}
\end{block}

## Invertibility

* Any MA($q$) process can be written as an AR($\infty$) process if we impose some constraints on the MA parameters.
* Then the MA model is called "invertible".
* Invertible models have some mathematical properties that make them easier to use in practice.
* Invertibility of an ARIMA model is equivalent to forecastability of an ETS model.

# 9.5 - Non-seasonal ARIMA models

If we combine differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for AutoRegressive Integrated Moving Average (in this context, “integration” is the reverse of differencing).

The full model can be written as :

\begin{block}{Autoregressive Moving Average models:}
\begin{align*}
  y_{t} &= c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p} \\
        & \hspace*{2.4cm}\text{} + \theta_{1}\varepsilon_{t - 1} + \cdots + \theta_{q}\varepsilon_{t - q} + \varepsilon_{t}.
\end{align*}
\end{block}\pause

